# Traning: WWW-Robots

In this little training challenge, you are going to learn about the Robots\_exclusion\_standard.

The robots.txt file is used by web crawlers to check if they are allowed to crawl and index your website or only parts of it.

Sometimes these files reveal the directory structure instead protecting the content from being crawled.

Enjoy!

## 翻译

这个小训练里你将会学到robots协议。robots.txt是用来告诉爬虫网站的某些部分可不可以爬的。有时候这些文件泄漏了目录结构而非保护网站避免被爬走。

## 题解

直奔根目录寻找robots.txt
